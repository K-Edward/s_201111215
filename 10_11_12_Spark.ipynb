{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ[\"SPARK_HOME\"]=\"C:/Users/400T6B/Code/s_201111215/spark-2.0.0-bin-hadoop2.6\"\n",
    "os.environ[\"PYLIB\"]=os.path.join(os.environ[\"SPARK_HOME\"],'python','lib')\n",
    "sys.path.insert(0,os.path.join(os.environ[\"PYLIB\"],'py4j-0.10.1-src.zip'))\n",
    "sys.path.insert(0,os.path.join(os.environ[\"PYLIB\"],'pyspark.zip'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ[\"SPARK_HOME\"]=\"C:/Code/s_201111215/spark-2.0.0-bin-hadoop2.6\"\n",
    "os.environ[\"PYLIB\"]=os.path.join(os.environ[\"SPARK_HOME\"],'python','lib')\n",
    "sys.path.insert(0,os.path.join(os.environ[\"PYLIB\"],'py4j-0.10.1-src.zip'))\n",
    "sys.path.insert(0,os.path.join(os.environ[\"PYLIB\"],'pyspark.zip'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ[\"SPARK_HOME\"]=\"C:/Users/88/Code/s_201111215/spark-2.0.0-bin-hadoop2.6\"\n",
    "os.environ[\"PYLIB\"]=os.path.join(os.environ[\"SPARK_HOME\"],'python','lib')\n",
    "sys.path.insert(0,os.path.join(os.environ[\"PYLIB\"],'py4j-0.10.1-src.zip'))\n",
    "sys.path.insert(0,os.path.join(os.environ[\"PYLIB\"],'pyspark.zip'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/400T6B/Code/s_201111215/spark-2.0.0-bin-hadoop2.6\\python\\lib\\pyspark.zip\n",
      "C:/Users/400T6B/Code/s_201111215/spark-2.0.0-bin-hadoop2.6\\python\\lib\\py4j-0.10.1-src.zip\n",
      "\n",
      "C:\\Users\\400T6B\\Anaconda2\\python27.zip\n",
      "C:\\Users\\400T6B\\Anaconda2\\DLLs\n",
      "C:\\Users\\400T6B\\Anaconda2\\lib\n",
      "C:\\Users\\400T6B\\Anaconda2\\lib\\plat-win\n",
      "C:\\Users\\400T6B\\Anaconda2\\lib\\lib-tk\n",
      "C:\\Users\\400T6B\\Anaconda2\n",
      "C:\\Users\\400T6B\\Anaconda2\\lib\\site-packages\n",
      "C:\\Users\\400T6B\\Anaconda2\\lib\\site-packages\\Sphinx-1.5.1-py2.7.egg\n",
      "C:\\Users\\400T6B\\Anaconda2\\lib\\site-packages\\win32\n",
      "C:\\Users\\400T6B\\Anaconda2\\lib\\site-packages\\win32\\lib\n",
      "C:\\Users\\400T6B\\Anaconda2\\lib\\site-packages\\Pythonwin\n",
      "C:\\Users\\400T6B\\Anaconda2\\lib\\site-packages\\setuptools-27.2.0-py2.7.egg\n",
      "C:\\Users\\400T6B\\Anaconda2\\lib\\site-packages\\IPython\\extensions\n",
      "C:\\Users\\400T6B\\.ipython\n"
     ]
    }
   ],
   "source": [
    "for i in sys.path:\n",
    "    print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "\n",
    "myConf=pyspark.SparkConf()\n",
    "spark = pyspark.sql.SparkSession.builder\\\n",
    "    .master(\"local\")\\\n",
    "    .appName(\"myApp\")\\\n",
    "    .config(conf=myConf)\\\n",
    "    .config('spark.sql.warehouse.dir', 'file:///C:/Users/400T6B/Code/s_201111215/data')\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: java.lang.NullPointerException\r\n\tat java.lang.ProcessBuilder.start(Unknown Source)\r\n\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:482)\r\n\tat org.apache.hadoop.util.Shell.run(Shell.java:455)\r\n\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:715)\r\n\tat org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:873)\r\n\tat org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:853)\r\n\tat org.apache.spark.util.Utils$.fetchFile(Utils.scala:471)\r\n\tat org.apache.spark.SparkContext.addFile(SparkContext.scala:1435)\r\n\tat org.apache.spark.SparkContext.addFile(SparkContext.scala:1384)\r\n\tat org.apache.spark.SparkContext$$anonfun$12.apply(SparkContext.scala:461)\r\n\tat org.apache.spark.SparkContext$$anonfun$12.apply(SparkContext.scala:461)\r\n\tat scala.collection.immutable.List.foreach(List.scala:381)\r\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:461)\r\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)\r\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)\r\n\tat java.lang.reflect.Constructor.newInstance(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:240)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:236)\r\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\r\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:211)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-773fb18ad3ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmyConf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSparkConf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mspark\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSparkSession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuilder\u001b[0m    \u001b[1;33m.\u001b[0m\u001b[0mmaster\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"local\"\u001b[0m\u001b[1;33m)\u001b[0m    \u001b[1;33m.\u001b[0m\u001b[0mappName\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"myApp\"\u001b[0m\u001b[1;33m)\u001b[0m    \u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmyConf\u001b[0m\u001b[1;33m)\u001b[0m    \u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'spark.sql.warehouse.dir'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'file:///C:/Users/88/Code/s_201111215/data'\u001b[0m\u001b[1;33m)\u001b[0m    \u001b[1;33m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mC:\\Users\\400T6B\\Code\\s_201111215\\spark-2.0.0-bin-hadoop2.6\\python\\lib\\pyspark.zip\\pyspark\\sql\\session.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    167\u001b[0m                     \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_options\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m                         \u001b[0msparkConf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m                     \u001b[0msc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msparkConf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m                     \u001b[1;31m# This SparkContext may be an existing one.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m                     \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_options\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\400T6B\\Code\\s_201111215\\spark-2.0.0-bin-hadoop2.6\\python\\lib\\pyspark.zip\\pyspark\\context.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[0;34m(cls, conf)\u001b[0m\n\u001b[1;32m    292\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m                 \u001b[0mSparkContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconf\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\400T6B\\Code\\s_201111215\\spark-2.0.0-bin-hadoop2.6\\python\\lib\\pyspark.zip\\pyspark\\context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n\u001b[0;32m--> 115\u001b[0;31m                           conf, jsc, profiler_cls)\n\u001b[0m\u001b[1;32m    116\u001b[0m         \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m             \u001b[1;31m# If an error occurs, clean up in order to allow future SparkContext creation:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\400T6B\\Code\\s_201111215\\spark-2.0.0-bin-hadoop2.6\\python\\lib\\pyspark.zip\\pyspark\\context.py\u001b[0m in \u001b[0;36m_do_init\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[1;31m# Create the Java SparkContext through Py4J\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jsc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjsc\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initialize_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m         \u001b[1;31m# Reset the SparkConf to the one actually used by the SparkContext in JVM.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_jconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\400T6B\\Code\\s_201111215\\spark-2.0.0-bin-hadoop2.6\\python\\lib\\pyspark.zip\\pyspark\\context.py\u001b[0m in \u001b[0;36m_initialize_context\u001b[0;34m(self, jconf)\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0mInitialize\u001b[0m \u001b[0mSparkContext\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfunction\u001b[0m \u001b[0mto\u001b[0m \u001b[0mallow\u001b[0m \u001b[0msubclass\u001b[0m \u001b[0mspecific\u001b[0m \u001b[0minitialization\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \"\"\"\n\u001b[0;32m--> 233\u001b[0;31m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mJavaSparkContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\400T6B\\Code\\s_201111215\\spark-2.0.0-bin-hadoop2.6\\python\\lib\\py4j-0.10.1-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1181\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1183\u001b[0;31m             answer, self._gateway_client, None, self._fqn)\n\u001b[0m\u001b[1;32m   1184\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\400T6B\\Code\\s_201111215\\spark-2.0.0-bin-hadoop2.6\\python\\lib\\py4j-0.10.1-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    310\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    311\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    313\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: java.lang.NullPointerException\r\n\tat java.lang.ProcessBuilder.start(Unknown Source)\r\n\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:482)\r\n\tat org.apache.hadoop.util.Shell.run(Shell.java:455)\r\n\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:715)\r\n\tat org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:873)\r\n\tat org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:853)\r\n\tat org.apache.spark.util.Utils$.fetchFile(Utils.scala:471)\r\n\tat org.apache.spark.SparkContext.addFile(SparkContext.scala:1435)\r\n\tat org.apache.spark.SparkContext.addFile(SparkContext.scala:1384)\r\n\tat org.apache.spark.SparkContext$$anonfun$12.apply(SparkContext.scala:461)\r\n\tat org.apache.spark.SparkContext$$anonfun$12.apply(SparkContext.scala:461)\r\n\tat scala.collection.immutable.List.foreach(List.scala:381)\r\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:461)\r\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)\r\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)\r\n\tat java.lang.reflect.Constructor.newInstance(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:240)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:236)\r\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\r\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:211)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "\n",
    "myConf=pyspark.SparkConf()\n",
    "spark = pyspark.sql.SparkSession.builder\\\n",
    "    .master(\"local\")\\\n",
    "    .appName(\"myApp\")\\\n",
    "    .config(conf=myConf)\\\n",
    "    .config('spark.sql.warehouse.dir', 'file:///C:/Users/88/Code/s_201111215/data')\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sys.path.remove('/home/jsl/Downloads/spark-1.6.0-bin-hadoop2.6/python/lib/py4j-0.9-src.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'2.0.0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\400T6B'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Home directory의 절대 주소(window 에서는 HOME을 지원하지 않기 때문에)\n",
    "os.path.expanduser(\"~\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n",
      "pyspark-shell\n",
      "local[*]\n",
      "117.16.43.175\n"
     ]
    }
   ],
   "source": [
    "print spark.version\n",
    "print spark.conf.get('spark.app.name')\n",
    "print spark.conf.get('spark.master')\n",
    "print spark.conf.get('spark.driver.host')\n",
    "#print spark.conf.get('spark.jars.packages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myList=[1,2,3,4,5,6,7]\n",
    "myRdd1 = spark.sparkContext.parallelize(myList) #sparkContext는 RDD읽을때 사용하는 것.\n",
    "myRdd1.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(myRdd1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRdd1.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting data/ds_spark_wiki.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile data/ds_spark_wiki.txt\n",
    "Wikipedia\n",
    "Apache Spark is an open source cluster computing framework.\n",
    "아파치 스파크는 오픈 소스 클러스터 컴퓨팅 프레임워크이다.\n",
    "Apache Spark Apache Spark Apache Spark Apache Spark\n",
    "아파치 스파크 아파치 스파크 아파치 스파크 아파치 스파크\n",
    "Originally developed at the University of California, Berkeley's AMPLab,\n",
    "the Spark codebase was later donated to the Apache Software Foundation,\n",
    "which has maintained it since.\n",
    "Spark provides an interface for programming entire clusters with\n",
    "implicit data parallelism and fault-tolerance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "myDf=spark.read.text(os.path.join(\"data\", \"ds_spark_wiki.txt\"))\n",
    "\n",
    "print type(myDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(value=u'Wikipedia')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myDf.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wikipedia\n"
     ]
    }
   ],
   "source": [
    "myRdd2=spark.sparkContext\\\n",
    "    .textFile(os.path.join(\"data\",\"ds_spark_wiki.txt\"))\n",
    "print myRdd2.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./data/ds_spark_2cols.csv\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./data/ds_spark_2cols.csv\n",
    "35, 2\n",
    "40, 27\n",
    "12, 38\n",
    "15, 31\n",
    "21, 1\n",
    "14, 19\n",
    "46, 1\n",
    "10, 34\n",
    "28, 3\n",
    "48, 1\n",
    "16, 2\n",
    "30, 3\n",
    "32, 2\n",
    "48, 1\n",
    "31, 2\n",
    "22, 1\n",
    "12, 3\n",
    "39, 29\n",
    "19, 37\n",
    "25, 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'35, 2', u'40, 27', u'12, 38', u'15, 31', u'21, 1']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRdd3 = spark.sparkContext\\\n",
    "    .textFile(os.path.join(\"data\",\"ds_spark_2cols.csv\"))\n",
    "myRdd3.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'35', u' 2'],\n",
       " [u'40', u' 27'],\n",
       " [u'12', u' 38'],\n",
       " [u'15', u' 31'],\n",
       " [u'21', u' 1']]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ',' <-얘를 기준으로 split하는 예제\n",
    "myRdd4 = myRdd3.map(lambda line: line.split(','))\n",
    "myRdd4.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[102.56, 97.7, 99.14, 100.03999999999999]\n"
     ]
    }
   ],
   "source": [
    "# 섭씨 온도를 화씨 온도로 바꿈\n",
    "celsius = [39.2, 36.5, 37.3, 37.8]\n",
    "def c2f(c):\n",
    "    f = list()\n",
    "    for i in c:\n",
    "        _f = (float(9)/5)*i + 32\n",
    "        f.append(_f)\n",
    "    return f\n",
    "\n",
    "print c2f(celsius)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[102.56, 97.7, 99.14, 100.03999999999999]\n"
     ]
    }
   ],
   "source": [
    "celsius = [39.2, 36.5, 37.3, 37.8]\n",
    "def c2f(c):\n",
    "    return (float(9)/5) * c + 32\n",
    "\n",
    "f = map(c2f, celsius)\n",
    "print f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[102.56, 97.7, 99.14, 100.03999999999999]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map(lambda c:(float(9)/5) * c + 32, celsius)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'World']\n"
     ]
    }
   ],
   "source": [
    "sentence = 'Hello World'\n",
    "words = sentence.split()\n",
    "print words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['H'], ['e'], ['l'], ['l'], ['o'], [], ['W'], ['o'], ['r'], ['l'], ['d']]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"Hello World\" #이렇게 넣으면 하나씩 가져가기 때문에 한글자씩 분해\n",
    "map(lambda x:x.split(),sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Hello', 'World']]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = [\"Hello World\"] #제대로 분리하려면 리스트에 넣어서 넘겨야 한다.\n",
    "map(lambda x:x.split(),sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 3, 5, 13, 21, 55]\n"
     ]
    }
   ],
   "source": [
    "fib = [0,1,1,2,3,5,8,13,21,34,55]\n",
    "result = filter(lambda x: x % 2, fib)\n",
    "print result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5050"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduce(lambda x, y: x+y, range(1,101))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nRdd = spark.sparkContext.parallelize([1, 2, 3, 4])\n",
    "squared = nRdd.map(lambda x: x * x)\n",
    "#얘는 변환역할이라서 결과 금방 나옴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 4, 9, 16]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squared.collect() #얘는 action 이라서 실행 결과가 좀 늦음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5050"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRdd100 = spark.sparkContext.parallelize(range(1,101))\n",
    "myRdd100.reduce(lambda x,y : x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 1 4 1.11803398875 1.25\n"
     ]
    }
   ],
   "source": [
    "print nRdd.sum(), nRdd.min(), nRdd.max(), nRdd.stdev(), nRdd.variance() #행이 아닌 열"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#2017.05.22\n",
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 공백문자로 split하는 예제\n",
    "# split의 결과를 words에 저장\n",
    "words = myRdd2.map(lambda x:x.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# split 의 결과가 10줄임.\n",
    "words.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def mySplit(x):\n",
    "    return x.split(\" \")\n",
    "\n",
    "words = myRdd2.map(mySplit)\n",
    "\n",
    "words.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# [] <- 이런 대괄호로 묶인 걸 take(3)로 3개 가져옴\n",
    "words.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for line in words.collect():\n",
    "    for word in line:\n",
    "        print word,\n",
    "    print \"\\n-------------\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# myRdd2 를 map하면 transform 한다는 얘기\n",
    "# 각 문장의 길이를 숫자로 표현 ex) 첫째줄은 9글자 둘째줄은 59글자..\n",
    "myRdd2.map(lambda s:len(s)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# spark 가 들어간 문장을 filter 하는 예제\n",
    "myRdd_spark = myRdd2.filter(lambda line: \"Spark\" in line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# count는 action함수\n",
    "# spark가 들어간 문장은 4개\n",
    "print myRdd_spark.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 한글 예제\n",
    "# 한글로 \"스파크\" 가 들어간 문장을 filter\n",
    "myRdd_unicode = myRdd2.filter(lambda line:u\"스파크\" in line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# \"스파크\"가 들어간 첫번째(first)문장을 print\n",
    "print myRdd_unicode.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stopwords = ['is', 'am', 'are', 'the', 'for', 'a']\n",
    "myRdd_stop = myRdd2.filter(lambda x:x not in stopwords)\n",
    "myRdd_stop.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a = [\"this is\", \"a line\"]\n",
    "_rdd = spark.sparkContext.parallelize(a)\n",
    "words = _rdd.map(lambda x:x.split( ))\n",
    "print words.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# _upper에다가 _rdd에 저장되어 있던 \"this is\", \"a line\"을 map 으로 transform 함\n",
    "# replace 를 이용해서 a 를 AA 로 바꿈\n",
    "# _upper 출력 (take(10)은 잘 모름)\n",
    "_upper=_rdd.map(lambda x:x.replace(\"a\",\"AA\"))\n",
    "_upper.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%writefile data/ds_spark_wiki2.txt\n",
    "SPARK is a formally defined computer programming language based on the Ada programming language, intended for the development of high integrity software used in systems where predictable and highly reliable operation is essential. It facilitates the development of applications that demand safety, security, or business integrity.\n",
    "Originally, there were three versions of the SPARK language (SPARK83, SPARK95, SPARK2005) based on Ada 83, Ada 95 and Ada 2005 respectively.\n",
    "A fourth version of the SPARK language, SPARK 2014, based on Ada 2012, was released on April 30, 2014. SPARK 2014 is a complete re-design of the language and supporting verification tools.\n",
    "The SPARK language consists of a well-defined subset of the Ada language that uses contracts to describe the specification of components in a form that is suitable for both static and dynamic verification.\n",
    "In SPARK83/95/2005, the contracts are encoded in Ada comments (and so are ignored by any standard Ada compiler), but are processed by the SPARK \"Examiner\" and its associated tools.\n",
    "SPARK 2014, in contrast, uses Ada 2012's built-in \"aspect\" syntax to express contracts, bringing them into the core of the language. The main tool for SPARK 2014 (GNATprove) is based on the GNAT/GCC infrastructure, and re-uses almost the entirety of the GNAT Ada 2012 front-end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15, u'the')\n",
      "(10, u'Ada')\n",
      "(10, u'of')\n",
      "(9, u'SPARK')\n",
      "(8, u'language')\n",
      "(6, u'and')\n",
      "(5, u'is')\n",
      "(5, u'2014')\n",
      "(5, u'on')\n",
      "(4, u'based')\n",
      "(4, u'a')\n",
      "(4, u'in')\n",
      "(3, u'for')\n",
      "(3, u'contracts')\n",
      "(3, u'are')\n",
      "(3, u'that')\n",
      "(2, u'development')\n",
      "(2, u'tools')\n",
      "(2, u'integrity')\n",
      "(2, u'The')\n",
      "(2, u'uses')\n",
      "(2, u'programming')\n",
      "(2, u'to')\n",
      "(2, u'2012')\n",
      "(2, u'by')\n",
      "(2, u'verification')\n",
      "(1, u'subset')\n",
      "(1, u'associated')\n",
      "(1, u'defined')\n",
      "(1, u'dynamic')\n",
      "(1, u'high')\n",
      "(1, u'front-end')\n",
      "(1, u'well-defined')\n",
      "(1, u'consists')\n",
      "(1, u'facilitates')\n",
      "(1, u'operation')\n",
      "(1, u'business')\n",
      "(1, u'specification')\n",
      "(1, u'comments')\n",
      "(1, u'GNAT/GCC')\n",
      "(1, u'almost')\n",
      "(1, u'entirety')\n",
      "(1, u'safety')\n",
      "(1, u'fourth')\n",
      "(1, u'(GNATprove)')\n",
      "(1, u'SPARK83/95/2005')\n",
      "(1, u'was')\n",
      "(1, u'into')\n",
      "(1, u'contrast')\n",
      "(1, u'A')\n",
      "(1, u'GNAT')\n",
      "(1, u'them')\n",
      "(1, u'formally')\n",
      "(1, u'form')\n",
      "(1, u'built-in')\n",
      "(1, u'Originally')\n",
      "(1, u'tool')\n",
      "(1, u'\"aspect\"')\n",
      "(1, u'bringing')\n",
      "(1, u'predictable')\n",
      "(1, u'but')\n",
      "(1, u'95')\n",
      "(1, u'(SPARK83')\n",
      "(1, u'reliable')\n",
      "(1, u'so')\n",
      "(1, u'components')\n",
      "(1, u'static')\n",
      "(1, u'security')\n",
      "(1, u'where')\n",
      "(1, u'SPARK95')\n",
      "(1, u'ignored')\n",
      "(1, u\"2012's\")\n",
      "(1, u'30')\n",
      "(1, u'core')\n",
      "(1, u'highly')\n",
      "(1, u're-uses')\n",
      "(1, u'computer')\n",
      "(1, u'83')\n",
      "(1, u'In')\n",
      "(1, u'any')\n",
      "(1, u'intended')\n",
      "(1, u'SPARK2005)')\n",
      "(1, u'(and')\n",
      "(1, u'processed')\n",
      "(1, u'there')\n",
      "(1, u'three')\n",
      "(1, u're-design')\n",
      "(1, u'version')\n",
      "(1, u'systems')\n",
      "(1, u'main')\n",
      "(1, u'\"Examiner\"')\n",
      "(1, u'supporting')\n",
      "(1, u'used')\n",
      "(1, u'complete')\n",
      "(1, u'infrastructure')\n",
      "(1, u'respectively')\n",
      "(1, u'were')\n",
      "(1, u'describe')\n",
      "(1, u'standard')\n",
      "(1, u'April')\n",
      "(1, u'demand')\n",
      "(1, u'encoded')\n",
      "(1, u'released')\n",
      "(1, u'both')\n",
      "(1, u'compiler)')\n",
      "(1, u'applications')\n",
      "(1, u'syntax')\n",
      "(1, u'versions')\n",
      "(1, u'It')\n",
      "(1, u'essential')\n",
      "(1, u'express')\n",
      "(1, u'2005')\n",
      "(1, u'suitable')\n",
      "(1, u'its')\n",
      "(1, u'or')\n",
      "(1, u'software')\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAcEAAAD8CAYAAAAPKB8vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nP1IQlWVeEvI0AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x82aecf8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#%%writefile src/ds_spark_wiki_wordcount.py\n",
    "#import os\n",
    "#import sys\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# os.environ[\"SPARK_HOME\"]=\"C:/Users/400T6B/Code/s_201111215/spark-2.0.0-bin-hadoop2.6\"\n",
    "# os.environ[\"PYLIB\"]=os.path.join(os.environ[\"SPARK_HOME\"],'python','lib')\n",
    "# sys.path.insert(0,os.path.join(os.environ[\"PYLIB\"],'py4j-0.10.1-src.zip'))\n",
    "# sys.path.insert(0,os.path.join(os.environ[\"PYLIB\"],'pyspark.zip'))\n",
    "\n",
    "\n",
    "import pyspark\n",
    "\n",
    "\n",
    "def doIt():\n",
    "    my_Rdd=spark.sparkContext.textFile(os.path.join(\"data\",\"ds_spark_wiki2.txt\"))\n",
    "    #print my_Rdd.first()\n",
    "\n",
    "    my_words = my_Rdd.map(lambda x:x.split(' '))\n",
    "\n",
    "    #my_words2.count()\n",
    "\n",
    "    #my_words.take(5)\n",
    "\n",
    "    my_Rdd1 = my_Rdd.map(lambda x:x.replace(\",\",\"\"))\n",
    "    my_Rdd2 = my_Rdd1.map(lambda x:x.replace(\".\",\"\"))\n",
    "\n",
    "    #word_count = my_Rdd2.flatMap(lambda x:x.split( )).map(lambda x:(x,1)).groupByKey().mapValues(sum).sortByKey(True).take(200)\n",
    "\n",
    "    #for word in word_count:\n",
    "    #    print word\n",
    "\n",
    "    word_count2 = my_Rdd2.flatMap(lambda x:x.split()).map(lambda x:(x,1)).reduceByKey(lambda x,y:x+y).map(lambda x:(x[1],x[0])).sortByKey(False).take(200)\n",
    "\n",
    "    for word2 in word_count2:\n",
    "        print word2\n",
    "\n",
    "    count = map(lambda x:x[0], word_count2) # x\n",
    "    words = map(lambda x:x[1], word_count2) # y\n",
    "    plt.barh(range(len(count)), count, color = 'red')\n",
    "    plt.yticks(range(len(count)), words)\n",
    "    plt.show()\n",
    "    \n",
    "if __name__ == \"__main__\":    \n",
    "    myConf=pyspark.SparkConf()\n",
    "    spark = pyspark.sql.SparkSession.builder\\\n",
    "        .master(\"local\")\\\n",
    "        .appName(\"myApp\")\\\n",
    "        .config(conf=myConf)\\\n",
    "        .config('spark.sql.warehouse.dir', 'file:///C:/Users/400T6B/Code/s_201111215/data')\\\n",
    "        .getOrCreate()\n",
    "    doIt()\n",
    "    spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%writefile src/ds_spark_rdd_hello.py\n",
    "#!/usr/bin/env python\n",
    "# -*- coding: UTF-8 -*-\n",
    "import pyspark\n",
    "\n",
    "def doIt():\n",
    "    print \"---------RESULT-----------\"\n",
    "    print spark.version\n",
    "    spark.conf.set(\"spark.logConf\",\"false\")\n",
    "    rdd=spark.sparkContext.parallelize(range(1000), 10)\n",
    "    print \"mean=\",rdd.mean()\n",
    "    nums = spark.sparkContext.parallelize([1, 2, 3, 4])\n",
    "    squared = nums.map(lambda x: x * x).collect()\n",
    "    for num in squared:\n",
    "        print \"%i \" % (num)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    myConf=pyspark.SparkConf()\n",
    "    spark = pyspark.sql.SparkSession.builder\\\n",
    "        .master(\"local\")\\\n",
    "        .appName(\"myApp\")\\\n",
    "        .config(conf=myConf)\\\n",
    "        .getOrCreate()\n",
    "    doIt()\n",
    "    spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.feature import HashingTF\n",
    "\n",
    "documents = spark.sparkContext.textFile(\"data/ds_spark_wiki.txt\")\\\n",
    "    .map(lambda line: line.split(\" \"))\n",
    "\n",
    "\n",
    "hashingTF = HashingTF()\n",
    "tf = hashingTF.transform(documents)\n",
    "tf.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SparseVector의 역할은 \n",
    "공간을 너무 많이 차지해서 Key : Value 만으로 줄여 놓는 것."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "myList = list()\n",
    "print myList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "myList=[('1','kim, js',170),\n",
    "        ('1','lee, sm', 175),\n",
    "        ('2','lim, yg',180),\n",
    "        ('2','lee',170)]\n",
    "myDf=spark.createDataFrame(myList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: string (nullable = true)\n",
      " |-- _2: string (nullable = true)\n",
      " |-- _3: long (nullable = true)\n",
      "\n",
      "[Row(_1=u'1', _2=u'kim, js', _3=170)]\n"
     ]
    }
   ],
   "source": [
    "myDf.printSchema()\n",
    "print myDf.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(year=u'1', name=u'kim, js', height=170)]\n"
     ]
    }
   ],
   "source": [
    "print spark.createDataFrame(myList, ['year','name','height']).take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+\n",
      "|name|      item|\n",
      "+----+----------+\n",
      "| kim|  espresso|\n",
      "| lee|     latte|\n",
      "| lee| americano|\n",
      "| lim|  affocato|\n",
      "| kim|long black|\n",
      "| lee|  espresso|\n",
      "| lee|     latte|\n",
      "| lim| americano|\n",
      "| kim|  affocato|\n",
      "| lee|long black|\n",
      "+----+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "names = [\"kim\",\"lee\",\"lee\",\"lim\"]\n",
    "items = [\"espresso\",\"latte\",\"americano\",\"affocato\",\"long black\",\"macciato\"]\n",
    "df = spark.createDataFrame([(names[i%4], items[i%5]) for i in range(100)],\\\n",
    "                           [\"name\",\"item\"])\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|3chars|\n",
      "+------+\n",
      "|   esp|\n",
      "|   lat|\n",
      "|   ame|\n",
      "+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.item.substr(1, 3).alias(\"3chars\")).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row1:  1 kim, js\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "Person = Row('year','name', 'height')\n",
    "row1=Person('1','kim, js',170)\n",
    "print \"row1: \",row1.year, row1.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "myRows = [row1,\n",
    "          Person('1','lee, sm', 175),\n",
    "          Person('2','lim, yg',180),\n",
    "          Person('2','lee',170)]\n",
    "\n",
    "myDf=spark.createDataFrame(myRows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- height: long (nullable = true)\n",
      "\n",
      "None\n",
      "+----+-------+------+\n",
      "|year|   name|height|\n",
      "+----+-------+------+\n",
      "|   1|kim, js|   170|\n",
      "|   1|lee, sm|   175|\n",
      "|   2|lim, yg|   180|\n",
      "|   2|    lee|   170|\n",
      "+----+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print myDf.printSchema()\n",
    "myDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- height: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(year=u'1', name=u'kim, js', height=170)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField\n",
    "from pyspark.sql.types import StringType, IntegerType\n",
    "mySchema=StructType([\n",
    "    StructField(\"year\", StringType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"height\", IntegerType(), True)\n",
    "])\n",
    "myDf=spark.createDataFrame(myRows, mySchema)\n",
    "myDf.printSchema()\n",
    "myDf.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "myList=[('1','kim, js',170),('1','lee, sm', 175),('2','lim, yg',180),('2','lee',170)]\n",
    "myRdd = spark.sparkContext.parallelize(myList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: string (nullable = true)\n",
      " |-- _2: string (nullable = true)\n",
      " |-- _3: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rddDf=myRdd.toDF()\n",
    "rddDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: string (nullable = true)\n",
      " |-- _2: string (nullable = true)\n",
      " |-- _3: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rddDf=spark.createDataFrame(myRdd)\n",
    "rddDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "| _1|     _2|\n",
      "+---+-------+\n",
      "|  1|kim, js|\n",
      "|  2|    lee|\n",
      "+---+-------+\n",
      "\n",
      "+---+-------+\n",
      "| _1|max(_3)|\n",
      "+---+-------+\n",
      "|  1|    175|\n",
      "|  2|    180|\n",
      "+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rddDf.where(rddDf._3 < 175)\\\n",
    "    .select([rddDf._1, rddDf._2]).show()\n",
    "rddDf.groupby(rddDf._1).max().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- height: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- year: long (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(height=170, name=u'kim, js', year=1)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_myRdd=myRdd.map(lambda x:Row(year=int(x[0]),name=x[1],height=int(x[2])))\n",
    "_myDf=spark.createDataFrame(_myRdd)\n",
    "_myDf.printSchema()\n",
    "_myDf.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(age=10, name='js1'), Row(age=20, name='js2')]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import StructType, StructField\n",
    "from pyspark.sql.types import StringType, IntegerType, TimestampType\n",
    "r1=Row(name=\"js1\",age=10)\n",
    "r2=Row(name=\"js2\",age=20)\n",
    "_myRdd=spark.sparkContext.parallelize([r1,r2])\n",
    "_myRdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n",
      "+---+----+\n",
      "|age|name|\n",
      "+---+----+\n",
      "| 10| js1|\n",
      "| 20| js2|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema=StructType([\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    #StructField(\"created\", TimestampType(), True)\n",
    "])\n",
    "_myDf=spark.createDataFrame(_myRdd,schema)\n",
    "_myDf.printSchema()\n",
    "_myDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- height: double (nullable = true)\n",
      "\n",
      "+---+----+------+\n",
      "| id|name|height|\n",
      "+---+----+------+\n",
      "|  1| kim|  50.0|\n",
      "|  2| lee|  60.0|\n",
      "|  3|park|  70.0|\n",
      "+---+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "myRdd=spark.sparkContext.parallelize([(1, 'kim', 50.0), (2, 'lee', 60.0), (3, 'park', 70.0)])\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"height\", DoubleType(), True)\n",
    "])\n",
    "_myDf = spark.createDataFrame(myRdd, schema)\n",
    "_myDf.printSchema()\n",
    "_myDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>name</th>\n",
       "      <th>height</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>kim, js</td>\n",
       "      <td>170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>lee, sm</td>\n",
       "      <td>175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>lim, yg</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>lee</td>\n",
       "      <td>170</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  year     name  height\n",
       "0    1  kim, js     170\n",
       "1    1  lee, sm     175\n",
       "2    2  lim, yg     180\n",
       "3    2      lee     170"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myDf.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: java.lang.NullPointerException\r\n\tat java.lang.ProcessBuilder.start(Unknown Source)\r\n\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:482)\r\n\tat org.apache.hadoop.util.Shell.run(Shell.java:455)\r\n\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:715)\r\n\tat org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:873)\r\n\tat org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:853)\r\n\tat org.apache.spark.util.Utils$.fetchFile(Utils.scala:471)\r\n\tat org.apache.spark.SparkContext.addFile(SparkContext.scala:1435)\r\n\tat org.apache.spark.SparkContext.addFile(SparkContext.scala:1384)\r\n\tat org.apache.spark.SparkContext$$anonfun$12.apply(SparkContext.scala:461)\r\n\tat org.apache.spark.SparkContext$$anonfun$12.apply(SparkContext.scala:461)\r\n\tat scala.collection.immutable.List.foreach(List.scala:381)\r\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:461)\r\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)\r\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)\r\n\tat java.lang.reflect.Constructor.newInstance(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:240)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:236)\r\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\r\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:211)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-c92ea3658d83>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mmyConf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSparkConf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mspark\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSparkSession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuilder\u001b[0m        \u001b[1;33m.\u001b[0m\u001b[0mmaster\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"local\"\u001b[0m\u001b[1;33m)\u001b[0m        \u001b[1;33m.\u001b[0m\u001b[0mappName\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"myApp\"\u001b[0m\u001b[1;33m)\u001b[0m        \u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmyConf\u001b[0m\u001b[1;33m)\u001b[0m        \u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'spark.sql.warehouse.dir'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'file:///C:/Users/400T6B/Code/s_201111215/data'\u001b[0m\u001b[1;33m)\u001b[0m        \u001b[1;33m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mdo_It\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\400T6B\\Code\\s_201111215\\spark-2.0.0-bin-hadoop2.6\\python\\lib\\pyspark.zip\\pyspark\\sql\\session.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    167\u001b[0m                     \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_options\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m                         \u001b[0msparkConf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m                     \u001b[0msc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msparkConf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m                     \u001b[1;31m# This SparkContext may be an existing one.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m                     \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_options\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\400T6B\\Code\\s_201111215\\spark-2.0.0-bin-hadoop2.6\\python\\lib\\pyspark.zip\\pyspark\\context.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[0;34m(cls, conf)\u001b[0m\n\u001b[1;32m    292\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m                 \u001b[0mSparkContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconf\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\400T6B\\Code\\s_201111215\\spark-2.0.0-bin-hadoop2.6\\python\\lib\\pyspark.zip\\pyspark\\context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n\u001b[0;32m--> 115\u001b[0;31m                           conf, jsc, profiler_cls)\n\u001b[0m\u001b[1;32m    116\u001b[0m         \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m             \u001b[1;31m# If an error occurs, clean up in order to allow future SparkContext creation:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\400T6B\\Code\\s_201111215\\spark-2.0.0-bin-hadoop2.6\\python\\lib\\pyspark.zip\\pyspark\\context.py\u001b[0m in \u001b[0;36m_do_init\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[1;31m# Create the Java SparkContext through Py4J\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jsc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjsc\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initialize_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m         \u001b[1;31m# Reset the SparkConf to the one actually used by the SparkContext in JVM.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_jconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\400T6B\\Code\\s_201111215\\spark-2.0.0-bin-hadoop2.6\\python\\lib\\pyspark.zip\\pyspark\\context.py\u001b[0m in \u001b[0;36m_initialize_context\u001b[0;34m(self, jconf)\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0mInitialize\u001b[0m \u001b[0mSparkContext\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfunction\u001b[0m \u001b[0mto\u001b[0m \u001b[0mallow\u001b[0m \u001b[0msubclass\u001b[0m \u001b[0mspecific\u001b[0m \u001b[0minitialization\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \"\"\"\n\u001b[0;32m--> 233\u001b[0;31m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mJavaSparkContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\400T6B\\Code\\s_201111215\\spark-2.0.0-bin-hadoop2.6\\python\\lib\\py4j-0.10.1-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1181\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1183\u001b[0;31m             answer, self._gateway_client, None, self._fqn)\n\u001b[0m\u001b[1;32m   1184\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\400T6B\\Code\\s_201111215\\spark-2.0.0-bin-hadoop2.6\\python\\lib\\py4j-0.10.1-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    310\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    311\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    313\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: java.lang.NullPointerException\r\n\tat java.lang.ProcessBuilder.start(Unknown Source)\r\n\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:482)\r\n\tat org.apache.hadoop.util.Shell.run(Shell.java:455)\r\n\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:715)\r\n\tat org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:873)\r\n\tat org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:853)\r\n\tat org.apache.spark.util.Utils$.fetchFile(Utils.scala:471)\r\n\tat org.apache.spark.SparkContext.addFile(SparkContext.scala:1435)\r\n\tat org.apache.spark.SparkContext.addFile(SparkContext.scala:1384)\r\n\tat org.apache.spark.SparkContext$$anonfun$12.apply(SparkContext.scala:461)\r\n\tat org.apache.spark.SparkContext$$anonfun$12.apply(SparkContext.scala:461)\r\n\tat scala.collection.immutable.List.foreach(List.scala:381)\r\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:461)\r\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)\r\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)\r\n\tat java.lang.reflect.Constructor.newInstance(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:240)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:236)\r\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\r\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:211)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n"
     ]
    }
   ],
   "source": [
    "#%%writefile src/ds_spark_wiki_wordcount2.py\n",
    "\n",
    "import os\n",
    "from operator import add\n",
    "import pyspark\n",
    "from pyspark.mllib.feature import HashingTF\n",
    "\n",
    "def do_It():    \n",
    "    #word_count = spark.sparkContext.textFile(os.path.join(\"data\",\"ds_spark_wiki2.txt\")).flatMap(lambda x: x.split(' ')).map(lambda x: (x.lower().rstrip().lstrip().rstrip(',').rstrip('.'), 1)).reduceByKey(add)\n",
    "    #word_count = spark.sparkContext.textFile(os.path.join(\"data\",\"ds_spark_wiki2.txt\")).map(lambda x: x.replace('.',' ').replace(',',' ').replace('/',' ').replace('-',' ').replace('\"',' ').replace('(',' ').replace(')',' ').lower()).map(lambda x:x.split()).map(lambda x:[(i,1) for i in x])\n",
    "    documents = spark.sparkContext.textFile(os.path.join(\"data\",\"ds_spark_wiki2.txt\")).map(lambda x: x.split(\" \"))\n",
    "    h_TF = HashingTF()\n",
    "    tf = h_TF.transform(documents)\n",
    "    tf.collect()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    myConf=pyspark.SparkConf()\n",
    "    spark = pyspark.sql.SparkSession.builder\\\n",
    "        .master(\"local\")\\\n",
    "        .appName(\"myApp\")\\\n",
    "        .config(conf=myConf)\\\n",
    "        .config('spark.sql.warehouse.dir', 'file:///C:/Users/400T6B/Code/s_201111215/data')\\\n",
    "        .getOrCreate()\n",
    "    do_It()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51\n",
      "(u'and', 1)\n"
     ]
    }
   ],
   "source": [
    "from operator import add\n",
    "wc = spark.sparkContext.textFile(os.path.join(\"data\",\"ds_spark_wiki.txt\"))\\\n",
    "    .flatMap(lambda x: x.split(' '))\\\n",
    "    .map(lambda x: (x.lower().rstrip().lstrip().rstrip(',').rstrip('.'), 1))\\\n",
    "    .reduceByKey(add)\n",
    "\n",
    "print wc.count()\n",
    "\n",
    "print wc.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(age=29, name=u'Michael'),\n",
       " Row(age=30, name=u'Andy'),\n",
       " Row(age=19, name=u'Justin')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "cfile= os.path.join(os.environ[\"SPARK_HOME\"],\\\n",
    "           \"examples/src/main/resources/people.txt\")\n",
    "lines = spark.sparkContext.textFile(cfile)\n",
    "parts = lines.map(lambda l: l.split(\",\"))\n",
    "people = parts.map(lambda p: Row(name=p[0], age=int(p[1].strip())))\n",
    "\n",
    "_myDf = spark.createDataFrame(people)\n",
    "_myDf.printSchema()\n",
    "_myDf.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting data/ds_spark.csv\n"
     ]
    }
   ],
   "source": [
    "%%writefile data/ds_spark.csv\n",
    "1,2,3,4\n",
    "11,22,33,44\n",
    "111,222,333,444"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+\n",
      "|  1|  2|  3|  4|\n",
      "+---+---+---+---+\n",
      "| 11| 22| 33| 44|\n",
      "|111|222|333|444|\n",
      "+---+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').load('data/ds_spark.csv')\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting data/ds_spark_heightweight.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile data/ds_spark_heightweight.txt\n",
    "1\t65.78\t112.99\n",
    "2\t71.52\t136.49\n",
    "3\t69.40\t153.03\n",
    "4\t68.22\t142.34\n",
    "5\t67.79\t144.30\n",
    "6\t68.70\t123.30\n",
    "7\t69.80\t141.49\n",
    "8\t70.01\t136.46\n",
    "9\t67.90\t112.37\n",
    "10\t66.78\t120.67\n",
    "11\t66.49\t127.45\n",
    "12\t67.62\t114.14\n",
    "13\t68.30\t125.61\n",
    "14\t67.12\t122.46\n",
    "15\t68.28\t116.09\n",
    "16\t71.09\t140.00\n",
    "17\t66.46\t129.50\n",
    "18\t68.65\t142.97\n",
    "19\t71.23\t137.90\n",
    "20\t67.13\t124.04\n",
    "21\t67.83\t141.28\n",
    "22\t68.88\t143.54\n",
    "23\t63.48\t97.90\n",
    "24\t68.42\t129.50\n",
    "25\t67.63\t141.85\n",
    "26\t67.21\t129.72\n",
    "27\t70.84\t142.42\n",
    "28\t67.49\t131.55\n",
    "29\t66.53\t108.33\n",
    "30\t65.44\t113.89\n",
    "31\t69.52\t103.30\n",
    "32\t65.81\t120.75\n",
    "33\t67.82\t125.79\n",
    "34\t70.60\t136.22\n",
    "35\t71.80\t140.10\n",
    "36\t69.21\t128.75\n",
    "37\t66.80\t141.80\n",
    "38\t67.66\t121.23\n",
    "39\t67.81\t131.35\n",
    "40\t64.05\t106.71\n",
    "41\t68.57\t124.36\n",
    "42\t65.18\t124.86\n",
    "43\t69.66\t139.67\n",
    "44\t67.97\t137.37\n",
    "45\t65.98\t106.45\n",
    "46\t68.67\t128.76\n",
    "47\t66.88\t145.68\n",
    "48\t67.70\t116.82\n",
    "49\t69.82\t143.62\n",
    "50\t69.09\t134.93"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "import os\n",
    "\n",
    "a = spark.sparkContext.textFile(os.path.join('data','ds_spark_heightweight.txt'))\n",
    "\n",
    "b = a.map(lambda x:x.split('\\t'))\n",
    "c = spark.createDataFrame(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: string (nullable = true)\n",
      " |-- _2: string (nullable = true)\n",
      " |-- _3: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "c.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_1=u'1', _2=u'65.78', _3=u'112.99'),\n",
       " Row(_1=u'2', _2=u'71.52', _3=u'136.49'),\n",
       " Row(_1=u'3', _2=u'69.40', _3=u'153.03'),\n",
       " Row(_1=u'4', _2=u'68.22', _3=u'142.34'),\n",
       " Row(_1=u'5', _2=u'67.79', _3=u'144.30'),\n",
       " Row(_1=u'6', _2=u'68.70', _3=u'123.30'),\n",
       " Row(_1=u'7', _2=u'69.80', _3=u'141.49'),\n",
       " Row(_1=u'8', _2=u'70.01', _3=u'136.46'),\n",
       " Row(_1=u'9', _2=u'67.90', _3=u'112.37'),\n",
       " Row(_1=u'10', _2=u'66.78', _3=u'120.67'),\n",
       " Row(_1=u'11', _2=u'66.49', _3=u'127.45'),\n",
       " Row(_1=u'12', _2=u'67.62', _3=u'114.14'),\n",
       " Row(_1=u'13', _2=u'68.30', _3=u'125.61'),\n",
       " Row(_1=u'14', _2=u'67.12', _3=u'122.46'),\n",
       " Row(_1=u'15', _2=u'68.28', _3=u'116.09'),\n",
       " Row(_1=u'16', _2=u'71.09', _3=u'140.00'),\n",
       " Row(_1=u'17', _2=u'66.46', _3=u'129.50'),\n",
       " Row(_1=u'18', _2=u'68.65', _3=u'142.97'),\n",
       " Row(_1=u'19', _2=u'71.23', _3=u'137.90'),\n",
       " Row(_1=u'20', _2=u'67.13', _3=u'124.04'),\n",
       " Row(_1=u'21', _2=u'67.83', _3=u'141.28'),\n",
       " Row(_1=u'22', _2=u'68.88', _3=u'143.54'),\n",
       " Row(_1=u'23', _2=u'63.48', _3=u'97.90'),\n",
       " Row(_1=u'24', _2=u'68.42', _3=u'129.50'),\n",
       " Row(_1=u'25', _2=u'67.63', _3=u'141.85'),\n",
       " Row(_1=u'26', _2=u'67.21', _3=u'129.72'),\n",
       " Row(_1=u'27', _2=u'70.84', _3=u'142.42'),\n",
       " Row(_1=u'28', _2=u'67.49', _3=u'131.55'),\n",
       " Row(_1=u'29', _2=u'66.53', _3=u'108.33'),\n",
       " Row(_1=u'30', _2=u'65.44', _3=u'113.89'),\n",
       " Row(_1=u'31', _2=u'69.52', _3=u'103.30'),\n",
       " Row(_1=u'32', _2=u'65.81', _3=u'120.75'),\n",
       " Row(_1=u'33', _2=u'67.82', _3=u'125.79'),\n",
       " Row(_1=u'34', _2=u'70.60', _3=u'136.22'),\n",
       " Row(_1=u'35', _2=u'71.80', _3=u'140.10'),\n",
       " Row(_1=u'36', _2=u'69.21', _3=u'128.75'),\n",
       " Row(_1=u'37', _2=u'66.80', _3=u'141.80'),\n",
       " Row(_1=u'38', _2=u'67.66', _3=u'121.23'),\n",
       " Row(_1=u'39', _2=u'67.81', _3=u'131.35'),\n",
       " Row(_1=u'40', _2=u'64.05', _3=u'106.71'),\n",
       " Row(_1=u'41', _2=u'68.57', _3=u'124.36'),\n",
       " Row(_1=u'42', _2=u'65.18', _3=u'124.86'),\n",
       " Row(_1=u'43', _2=u'69.66', _3=u'139.67'),\n",
       " Row(_1=u'44', _2=u'67.97', _3=u'137.37'),\n",
       " Row(_1=u'45', _2=u'65.98', _3=u'106.45'),\n",
       " Row(_1=u'46', _2=u'68.67', _3=u'128.76'),\n",
       " Row(_1=u'47', _2=u'66.88', _3=u'145.68'),\n",
       " Row(_1=u'48', _2=u'67.70', _3=u'116.82'),\n",
       " Row(_1=u'49', _2=u'69.82', _3=u'143.62'),\n",
       " Row(_1=u'50', _2=u'69.09', _3=u'134.93')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.take(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'list'> <type 'dict'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{u'Club': u'Club Atl\\xc3\\xa9tico Talleres de Remedios de Escalada',\n",
       " u'ClubCountry': u'Argentina',\n",
       " u'Competition': u'World Cup',\n",
       " u'DateOfBirth': u'1905-5-5',\n",
       " u'FullName': u'\\xc3ngel Bossio',\n",
       " u'IsCaptain': False,\n",
       " u'Number': u'',\n",
       " u'Position': u'GK',\n",
       " u'Team': u'Argentina',\n",
       " u'Year': 1930}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "r=requests.get(\"https://raw.githubusercontent.com/jokecamp/FootballData/master/World%20Cups/all-world-cup-players.json\")\n",
    "wc=r.json()\n",
    "\n",
    "print type(wc), type(wc[0])\n",
    "\n",
    "wc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "wc의 type은 list !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 createDataFrame의 인자로 list 타입이 넘어감"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Club: string (nullable = true)\n",
      " |-- ClubCountry: string (nullable = true)\n",
      " |-- Competition: string (nullable = true)\n",
      " |-- DateOfBirth: string (nullable = true)\n",
      " |-- FullName: string (nullable = true)\n",
      " |-- IsCaptain: boolean (nullable = true)\n",
      " |-- Number: string (nullable = true)\n",
      " |-- Position: string (nullable = true)\n",
      " |-- Team: string (nullable = true)\n",
      " |-- Year: long (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(Club=u'Club Atl\\xc3\\xa9tico Talleres de Remedios de Escalada', ClubCountry=u'Argentina', Competition=u'World Cup', DateOfBirth=u'1905-5-5', FullName=u'\\xc3ngel Bossio', IsCaptain=False, Number=u'', Position=u'GK', Team=u'Argentina', Year=1930)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wcDF = spark.createDataFrame(wc)\n",
    "wcDF.printSchema()\n",
    "wcDF.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Club=u'Club Atl\\xc3\\xa9tico Talleres de Remedios de Escalada', ClubCountry=u'Argentina', Competition=u'World Cup', DateOfBirth=u'1905-5-5', FullName=u'\\xc3ngel Bossio', IsCaptain=False, Number=u'', Position=u'GK', Team=u'Argentina', Year=1930),\n",
       " Row(Club=u'Quilmes Atl\\xc3\\xa9tico Club', ClubCountry=u'Argentina', Competition=u'World Cup', DateOfBirth=u'1908-10-23', FullName=u'Juan Botasso', IsCaptain=False, Number=u'', Position=u'GK', Team=u'Argentina', Year=1930),\n",
       " Row(Club=u'Boca Junio', ClubCountry=u'Argentina', Competition=u'World Cup', DateOfBirth=u'1907-2-23', FullName=u'Roberto Cherro', IsCaptain=False, Number=u'', Position=u'FW', Team=u'Argentina', Year=1930)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wcDF = spark.createDataFrame(wc)\n",
    "wcDF.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{u'Club': u'Club Atl\\xc3\\xa9tico Talleres de Remedios de Escalada',\n",
       "  u'ClubCountry': u'Argentina',\n",
       "  u'Competition': u'World Cup',\n",
       "  u'DateOfBirth': u'1905-5-5',\n",
       "  u'FullName': u'\\xc3ngel Bossio',\n",
       "  u'IsCaptain': False,\n",
       "  u'Number': u'',\n",
       "  u'Position': u'GK',\n",
       "  u'Team': u'Argentina',\n",
       "  u'Year': 1930},\n",
       " {u'Club': u'Quilmes Atl\\xc3\\xa9tico Club',\n",
       "  u'ClubCountry': u'Argentina',\n",
       "  u'Competition': u'World Cup',\n",
       "  u'DateOfBirth': u'1908-10-23',\n",
       "  u'FullName': u'Juan Botasso',\n",
       "  u'IsCaptain': False,\n",
       "  u'Number': u'',\n",
       "  u'Position': u'GK',\n",
       "  u'Team': u'Argentina',\n",
       "  u'Year': 1930},\n",
       " {u'Club': u'Boca Junio',\n",
       "  u'ClubCountry': u'Argentina',\n",
       "  u'Competition': u'World Cup',\n",
       "  u'DateOfBirth': u'1907-2-23',\n",
       "  u'FullName': u'Roberto Cherro',\n",
       "  u'IsCaptain': False,\n",
       "  u'Number': u'',\n",
       "  u'Position': u'FW',\n",
       "  u'Team': u'Argentina',\n",
       "  u'Year': 1930}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wcRdd = spark.sparkContext.parallelize(wc)\n",
    "wcRdd.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{u'Club': u'Club Atl\\xc3\\xa9tico Talleres de Remedios de Escalada',\n",
       "  u'ClubCountry': u'Argentina',\n",
       "  u'Competition': u'World Cup',\n",
       "  u'DateOfBirth': u'1905-5-5',\n",
       "  u'FullName': u'\\xc3ngel Bossio',\n",
       "  u'IsCaptain': False,\n",
       "  u'Number': u'',\n",
       "  u'Position': u'GK',\n",
       "  u'Team': u'Argentina',\n",
       "  u'Year': 1930},\n",
       " {u'Club': u'Quilmes Atl\\xc3\\xa9tico Club',\n",
       "  u'ClubCountry': u'Argentina',\n",
       "  u'Competition': u'World Cup',\n",
       "  u'DateOfBirth': u'1908-10-23',\n",
       "  u'FullName': u'Juan Botasso',\n",
       "  u'IsCaptain': False,\n",
       "  u'Number': u'',\n",
       "  u'Position': u'GK',\n",
       "  u'Team': u'Argentina',\n",
       "  u'Year': 1930},\n",
       " {u'Club': u'Boca Junio',\n",
       "  u'ClubCountry': u'Argentina',\n",
       "  u'Competition': u'World Cup',\n",
       "  u'DateOfBirth': u'1907-2-23',\n",
       "  u'FullName': u'Roberto Cherro',\n",
       "  u'IsCaptain': False,\n",
       "  u'Number': u'',\n",
       "  u'Position': u'FW',\n",
       "  u'Team': u'Argentina',\n",
       "  u'Year': 1930},\n",
       " {u'Club': u'Central Norte Tucum\\xc3',\n",
       "  u'ClubCountry': u'Argentina',\n",
       "  u'Competition': u'World Cup',\n",
       "  u'DateOfBirth': u'1907-2-23',\n",
       "  u'FullName': u'Alberto Chividini',\n",
       "  u'IsCaptain': False,\n",
       "  u'Number': u'',\n",
       "  u'Position': u'DF',\n",
       "  u'Team': u'Argentina',\n",
       "  u'Year': 1930},\n",
       " {u'Club': u'Club Atletico Estudiantil Porte\\xc3\\xb1o',\n",
       "  u'ClubCountry': u'Argentina',\n",
       "  u'Competition': u'World Cup',\n",
       "  u'DateOfBirth': u'1909-3-19',\n",
       "  u'FullName': u'',\n",
       "  u'IsCaptain': False,\n",
       "  u'Number': u'10',\n",
       "  u'Position': u'FW',\n",
       "  u'Team': u'Argentina',\n",
       "  u'Year': 1930}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wcRdd = spark.sparkContext.parallelize(wc)\n",
    "wcRdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{u'Club': u'Club Atl\\xc3\\xa9tico Talleres de Remedios de Escalada',\n",
       "  u'ClubCountry': u'Argentina',\n",
       "  u'Competition': u'World Cup',\n",
       "  u'DateOfBirth': u'1905-5-5',\n",
       "  u'FullName': u'\\xc3ngel Bossio',\n",
       "  u'IsCaptain': False,\n",
       "  u'Number': u'',\n",
       "  u'Position': u'GK',\n",
       "  u'Team': u'Argentina',\n",
       "  u'Year': 1930}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wcRdd = spark.sparkContext.parallelize(wc)\n",
    "wcRdd.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Club: string (nullable = true)\n",
      " |-- ClubCountry: string (nullable = true)\n",
      " |-- Competition: string (nullable = true)\n",
      " |-- DateOfBirth: string (nullable = true)\n",
      " |-- FullName: string (nullable = true)\n",
      " |-- IsCaptain: boolean (nullable = true)\n",
      " |-- Number: string (nullable = true)\n",
      " |-- Position: string (nullable = true)\n",
      " |-- Team: string (nullable = true)\n",
      " |-- Year: long (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(Club=u'Club Atl\\xc3\\xa9tico Talleres de Remedios de Escalada', ClubCountry=u'Argentina', Competition=u'World Cup', DateOfBirth=u'1905-5-5', FullName=u'\\xc3ngel Bossio', IsCaptain=False, Number=u'', Position=u'GK', Team=u'Argentina', Year=1930)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "wcSchema = StructType([\n",
    "    StructField(\"Club\", StringType(), True),\n",
    "    StructField(\"ClubCountry\", StringType(), True),\n",
    "    StructField(\"Competition\", StringType(), True),\n",
    "    StructField(\"DateOfBirth\", DateType(), True),\n",
    "    StructField(\"FullName\", StringType(), True),\n",
    "    StructField(\"IsCaptain\", BooleanType(), True),\n",
    "    StructField(\"Number\", IntegerType(), True),\n",
    "    StructField(\"Position\", StringType(), True),\n",
    "    StructField(\"Team\", StringType(), True),\n",
    "    StructField(\"Year\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "wcDF=spark.createDataFrame(wcRdd)\n",
    "wcDF.printSchema()\n",
    "wcDF.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1991-11-25 00:00:00\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "print datetime.strptime(\"11/25/1991\", '%m/%d/%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Club: string (nullable = true)\n",
      " |-- ClubCountry: string (nullable = true)\n",
      " |-- Competition: string (nullable = true)\n",
      " |-- DateOfBirth: string (nullable = true)\n",
      " |-- FullName: string (nullable = true)\n",
      " |-- IsCaptain: boolean (nullable = true)\n",
      " |-- Number: string (nullable = true)\n",
      " |-- Position: string (nullable = true)\n",
      " |-- Team: string (nullable = true)\n",
      " |-- Year: long (nullable = true)\n",
      " |-- DoB: date (nullable = true)\n",
      " |-- NumberInt: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(Club=u'Club Atl\\xc3\\xa9tico Talleres de Remedios de Escalada', ClubCountry=u'Argentina', Competition=u'World Cup', DateOfBirth=u'1905-5-5', FullName=u'\\xc3ngel Bossio', IsCaptain=False, Number=u'', Position=u'GK', Team=u'Argentina', Year=1930, DoB=datetime.date(1905, 5, 5), NumberInt=None)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wcDF=wcDF.withColumn('DoB', wcDF['DateOfBirth'].cast(DateType()))\n",
    "wcDF=wcDF.withColumn('NumberInt', wcDF['Number'].cast(\"integer\"))\n",
    "wcDF.printSchema()\n",
    "wcDF.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "|age|name|\n",
      "+---+----+\n",
      "| 30|Andy|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jfile= os.path.join(os.environ[\"SPARK_HOME\"],\\\n",
    "           \"examples/src/main/resources/people.json\")\n",
    "\n",
    "_myDf= spark.read.json(jfile)\n",
    "_myDf.filter(_myDf['age'] > 21).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "u'path file:/C:/Users/400T6B/Code/s_201111215/data/people.parquet already exists.;'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-3b4dfe181622>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0m_myDf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"data\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"people.parquet\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0m_pDf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"data\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"people.parquet\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0m_pDf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\400T6B\\Code\\s_201111215\\spark-2.0.0-bin-hadoop2.6\\python\\lib\\pyspark.zip\\pyspark\\sql\\readwriter.py\u001b[0m in \u001b[0;36mparquet\u001b[0;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[1;32m    615\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_opts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcompression\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 617\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    618\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0msince\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1.6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\400T6B\\Code\\s_201111215\\spark-2.0.0-bin-hadoop2.6\\python\\lib\\py4j-0.10.1-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m         return_value = get_return_value(\n\u001b[0;32m--> 933\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m    934\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\400T6B\\Code\\s_201111215\\spark-2.0.0-bin-hadoop2.6\\python\\lib\\pyspark.zip\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[1;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m': '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m': '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: u'path file:/C:/Users/400T6B/Code/s_201111215/data/people.parquet already exists.;'"
     ]
    }
   ],
   "source": [
    "_myDf.write.parquet(os.path.join(\"data\",\"people.parquet\"))\n",
    "_pDf=spark.read.parquet(os.path.join(\"data\",\"people.parquet\"))\n",
    "_pDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- height: integer (nullable = true)\n",
      "\n",
      "+----+-------+------+\n",
      "|year|   name|height|\n",
      "+----+-------+------+\n",
      "|   1|kim, js|   170|\n",
      "|   1|lee, sm|   175|\n",
      "|   2|lim, yg|   180|\n",
      "|   2|    lee|   170|\n",
      "+----+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.printSchema()\n",
    "myDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Club: string (nullable = true)\n",
      " |-- ClubNation: string (nullable = true)\n",
      " |-- Competition: string (nullable = true)\n",
      " |-- DateOfBirth: string (nullable = true)\n",
      " |-- FullName: string (nullable = true)\n",
      " |-- IsCaptain: boolean (nullable = true)\n",
      " |-- Number: string (nullable = true)\n",
      " |-- Position: string (nullable = true)\n",
      " |-- Team: string (nullable = true)\n",
      " |-- Year: long (nullable = true)\n",
      " |-- DoB: date (nullable = true)\n",
      " |-- NumberInt: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wcDF=wcDF.withColumnRenamed('ClubCountry','ClubNation')\n",
    "wcDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ClubCountry가 ClubNation으로 바뀌어 출력되었다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Club: string (nullable = true)\n",
      " |-- ClubNation: string (nullable = true)\n",
      " |-- Competition: string (nullable = true)\n",
      " |-- DateOfBirth: string (nullable = true)\n",
      " |-- FullName: string (nullable = true)\n",
      " |-- IsCaptain: boolean (nullable = true)\n",
      " |-- Number: string (nullable = true)\n",
      " |-- Position: string (nullable = true)\n",
      " |-- Team: string (nullable = true)\n",
      " |-- Year: long (nullable = true)\n",
      " |-- DoB: date (nullable = true)\n",
      " |-- NumberInt: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wcDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+----+\n",
      "|                Club|     Team|Year|\n",
      "+--------------------+---------+----+\n",
      "|Club AtlÃ©tico Ta...|Argentina|1930|\n",
      "|Quilmes AtlÃ©tico...|Argentina|1930|\n",
      "|          Boca Junio|Argentina|1930|\n",
      "|Central Norte TucumÃ|Argentina|1930|\n",
      "|Club Atletico Est...|Argentina|1930|\n",
      "|Racing Club de Av...|Argentina|1930|\n",
      "|     Sportivo Barrac|Argentina|1930|\n",
      "|          Boca Junio|Argentina|1930|\n",
      "|Estudiantes de La...|Argentina|1930|\n",
      "|Club AtlÃ©tico Sa...|Argentina|1930|\n",
      "+--------------------+---------+----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wcDF.createOrReplaceTempView(\"wc\")\n",
    "spark.sql(\"select Club,Team,Year from wc\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "wc라는 이름으로 가상 테이블을 만드는 과정.\n",
    "wc라는 이름은 임의로 다른 것으로 지어도 상관 없다.\n",
    "sql 문에서 from 뒤에 나오는 wc는 방금 만든 가상 테이블 명"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name=u'wc', database=None, description=None, tableType=u'TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "테이블을 여러개 만들어 놓았을 때 위 명령어를 통해서 관리 가능."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+---------+----+--------+\n",
      "|         FullName|                Club|     Team|Year|Position|\n",
      "+-----------------+--------------------+---------+----+--------+\n",
      "|     Ãngel Bossio|Club AtlÃ©tico Ta...|Argentina|1930|      GK|\n",
      "|     Juan Botasso|Quilmes AtlÃ©tico...|Argentina|1930|      GK|\n",
      "|   Roberto Cherro|          Boca Junio|Argentina|1930|      FW|\n",
      "|Alberto Chividini|Central Norte TucumÃ|Argentina|1930|      DF|\n",
      "|                 |Club Atletico Est...|Argentina|1930|      FW|\n",
      "|                 |Racing Club de Av...|Argentina|1930|      DF|\n",
      "|    Juan Evaristo|     Sportivo Barrac|Argentina|1930|      DF|\n",
      "|   Mario Evaristo|          Boca Junio|Argentina|1930|      FW|\n",
      "|  Manuel Ferreira|Estudiantes de La...|Argentina|1930|      FW|\n",
      "|       Luis Monti|Club AtlÃ©tico Sa...|Argentina|1930|      MF|\n",
      "+-----------------+--------------------+---------+----+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wcDF.createOrReplaceTempView(\"wc\")\n",
    "wcPlayers=spark.sql(\"select FullName,Club,Team,Year,Position from wc\")\n",
    "\n",
    "wcPlayers.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 예제 처럼 wc가 이미 있는데 다시 만든다고 하면\n",
    "1) 있을 경우 -> 있는 것 사용\n",
    "2) 없을 경우 -> 새로 만들어서 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "지금은 wcPlayers는 DataFrame이기 때문에 RDD로 바꿔서 출력한다.(아래 예제)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full name: Ãngel Bossio, Club: Club AtlÃ©tico Talleres de Remedios de Escalada\n",
      "Full name: Juan Botasso, Club: Quilmes AtlÃ©tico Club\n",
      "Full name: Roberto Cherro, Club: Boca Junio\n",
      "Full name: Alberto Chividini, Club: Central Norte TucumÃ\n",
      "Full name: , Club: Club Atletico Estudiantil PorteÃ±o\n",
      "Full name: , Club: Racing Club de Avellaneda\n",
      "Full name: Juan Evaristo, Club: Sportivo Barrac\n",
      "Full name: Mario Evaristo, Club: Boca Junio\n",
      "Full name: Manuel Ferreira, Club: Estudiantes de La Pla\n",
      "Full name: Luis Monti, Club: Club AtlÃ©tico San Lorenzo de Almagro\n",
      "Full name: , Club: Boca Junio\n",
      "Full name: Rodolfo Orlandini, Club: Club Atletico Estudiantil PorteÃ±o\n",
      "Full name: Fernando Paternoster, Club: Racing Club de Avellaneda\n",
      "Full name: Natalio Perinetti, Club: Racing Club de Avellaneda\n",
      "Full name: Carlos Peucelle, Club: Club Sportivo Buenos Aires\n",
      "Full name: Edmundo Piaggio, Club: Club AtlÃ©tico LanÃºs\n"
     ]
    }
   ],
   "source": [
    "namesRdd = wcPlayers.rdd.map(lambda x: \"Full name: \"+x[0] + \", Club: \" + x[1])\n",
    "for e in namesRdd.take(16):\n",
    "    print e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "현재 'sql문 : select FullName,Club,Team,Year from wc'이기 때문에 rdd.map(lambda x로 불러오는 x[0]은 첫번째 attribute인 FullName, x[1]은 두번째 attribute인 Club, x[2]는 Team, x[3]은 Year을 가져온다.\n",
    "\n",
    "아래에서 x[3]은 출력이 잘 안된다.\n",
    "\n",
    "x[4] 는 position 인데 출력이 되는걸보니 문자열 / 숫자의 차이인 것 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full name: Ãngel Bossio, Club: Club AtlÃ©tico Talleres de Remedios de Escalada, Team: Argentina, Position: GK\n",
      "Full name: Juan Botasso, Club: Quilmes AtlÃ©tico Club, Team: Argentina, Position: GK\n",
      "Full name: Roberto Cherro, Club: Boca Junio, Team: Argentina, Position: FW\n",
      "Full name: Alberto Chividini, Club: Central Norte TucumÃ, Team: Argentina, Position: DF\n",
      "Full name: , Club: Club Atletico Estudiantil PorteÃ±o, Team: Argentina, Position: FW\n",
      "Full name: , Club: Racing Club de Avellaneda, Team: Argentina, Position: DF\n",
      "Full name: Juan Evaristo, Club: Sportivo Barrac, Team: Argentina, Position: DF\n",
      "Full name: Mario Evaristo, Club: Boca Junio, Team: Argentina, Position: FW\n",
      "Full name: Manuel Ferreira, Club: Estudiantes de La Pla, Team: Argentina, Position: FW\n",
      "Full name: Luis Monti, Club: Club AtlÃ©tico San Lorenzo de Almagro, Team: Argentina, Position: MF\n",
      "Full name: , Club: Boca Junio, Team: Argentina, Position: DF\n",
      "Full name: Rodolfo Orlandini, Club: Club Atletico Estudiantil PorteÃ±o, Team: Argentina, Position: DF\n",
      "Full name: Fernando Paternoster, Club: Racing Club de Avellaneda, Team: Argentina, Position: DF\n",
      "Full name: Natalio Perinetti, Club: Racing Club de Avellaneda, Team: Argentina, Position: FW\n",
      "Full name: Carlos Peucelle, Club: Club Sportivo Buenos Aires, Team: Argentina, Position: FW\n",
      "Full name: Edmundo Piaggio, Club: Club AtlÃ©tico LanÃºs, Team: Argentina, Position: DF\n"
     ]
    }
   ],
   "source": [
    "namesRdd = wcPlayers.rdd.map(lambda x: \"Full name: \"+x[0] + \", Club: \" + x[1] + \", Team: \" + x[2] + \", Position: \" + x[4])\n",
    "for i in namesRdd.take(16):\n",
    "    print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\400T6B\\Code\\s_201111215\\data\\kddcup.data_10_percent.gz data does not exist! retrieving..\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import urllib\n",
    "_url = 'http://kdd.ics.uci.edu/databases/kddcup99/kddcup.data_10_percent.gz'\n",
    "_fname = os.path.join(os.getcwd(),'data','kddcup.data_10_percent.gz')\n",
    "if(not os.path.exists(_fname)):\n",
    "    print \"%s data does not exist! retrieving..\" % _fname\n",
    "    _f=urllib.urlretrieve(_url,_fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "urlretrieve 는 파일 다운받는 명령어\n",
    "\n",
    "-> 이미 있으면 안받고\n",
    "\n",
    "-> 없으면 받고"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다운로드 받은 kddcup.data_10_percent.gz의 압축을 풀어야 하는데\n",
    "\n",
    "spark에서 spark.sparkContext.textFile로 풀 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_rdd = spark.sparkContext.textFile(_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "494021"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'0,tcp,http,SF,181,5450,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,8,8,0.00,0.00,0.00,0.00,1.00,0.00,0.00,9,9,1.00,0.00,0.11,0.00,0.00,0.00,0.00,0.00,normal.',\n",
       " u'0,tcp,http,SF,239,486,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,8,8,0.00,0.00,0.00,0.00,1.00,0.00,0.00,19,19,1.00,0.00,0.05,0.00,0.00,0.00,0.00,0.00,normal.',\n",
       " u'0,tcp,http,SF,235,1337,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,8,8,0.00,0.00,0.00,0.00,1.00,0.00,0.00,29,29,1.00,0.00,0.03,0.00,0.00,0.00,0.00,0.00,normal.']"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#일단 3줄만 읽어봄\n",
    "_rdd.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lambda 함수는 익명함수 -> 이름을 주지 않고 쓰는 함수\n",
    "return 못함, 바로 명령"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97278\n"
     ]
    }
   ],
   "source": [
    "_normal = _rdd.filter(lambda x: 'normal.' in x)\n",
    "print _normal.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " , 로 구분 되어 있기 때문에 , 로 split 해준다.\n",
    " \n",
    " , 로 split 한 후엔 rdd로 만들어준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[u'0', u'tcp', u'http', u'SF', u'181', u'5450', u'0', u'0', u'0', u'0', u'0', u'1', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'8', u'8', u'0.00', u'0.00', u'0.00', u'0.00', u'1.00', u'0.00', u'0.00', u'9', u'9', u'1.00', u'0.00', u'0.11', u'0.00', u'0.00', u'0.00', u'0.00', u'0.00', u'normal.']]\n"
     ]
    }
   ],
   "source": [
    "_csvRdd=_rdd.map(lambda x: x.split(','))\n",
    "print _csvRdd.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(아래 예제)\n",
    "\n",
    "groupByKey랑 reduceByKey는 같다고 봐도 된다.\n",
    "\n",
    "41번째 요소를 이용해서 reduceByKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'guess_passwd.', 53),\n",
       " (u'nmap.', 231),\n",
       " (u'warezmaster.', 20),\n",
       " (u'rootkit.', 10),\n",
       " (u'warezclient.', 1020),\n",
       " (u'smurf.', 280790),\n",
       " (u'pod.', 264),\n",
       " (u'neptune.', 107201),\n",
       " (u'normal.', 97278),\n",
       " (u'spy.', 2),\n",
       " (u'ftp_write.', 8),\n",
       " (u'phf.', 4),\n",
       " (u'portsweep.', 1040),\n",
       " (u'teardrop.', 979),\n",
       " (u'buffer_overflow.', 30),\n",
       " (u'land.', 21),\n",
       " (u'imap.', 12),\n",
       " (u'loadmodule.', 9),\n",
       " (u'perl.', 3),\n",
       " (u'multihop.', 7),\n",
       " (u'back.', 2203),\n",
       " (u'ipsweep.', 1247),\n",
       " (u'satan.', 1589)]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_kv = _csvRdd.map(lambda x: (x[41], 1))\n",
    "_attack = _kv.reduceByKey(lambda x,y: x+y)\n",
    "\n",
    "_attack.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53\n"
     ]
    }
   ],
   "source": [
    "_normal = _rdd.filter(lambda x: 'guess_passwd.' in x)\n",
    "print _normal.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_normalRdd=_csvRdd.filter(lambda x: x[41]==\"normal.\")\n",
    "_attackRdd=_csvRdd.filter(lambda x: x[41]!=\"normal.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "normal이 아닌 것을 attack 이라고 생각한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97278\n",
      "396743\n"
     ]
    }
   ],
   "source": [
    "print _normalRdd.count()\n",
    "print _attackRdd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "두 개 더하면 총 갯수 였던 494021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "_csv = _rdd.map(lambda l: l.split(\",\"))\n",
    "_csvRdd = _csv.map(lambda p: \n",
    "    Row(\n",
    "        duration=int(p[0]), \n",
    "        protocol=p[1],\n",
    "        service=p[2],\n",
    "        flag=p[3],\n",
    "        src_bytes=int(p[4]),\n",
    "        dst_bytes=int(p[5])\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각 행별로 0번째, 1번째, 2번째, 3번째, 4번째, 5번째를 가지고 row를 만든다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
